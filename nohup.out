Matplotlib created a temporary config/cache directory at /tmp/matplotlib-gwuncxh9 because the default path (/home/20/xuanming/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Training the model on task 1
Initializing omega values for layer features.8.weight
Initializing omega values for layer features.8.bias
Initializing omega values for layer features.10.weight
Initializing omega values for layer features.10.bias
Initializing omega values for layer classifier.1.weight
Initializing omega values for layer classifier.1.bias
Initializing omega values for layer classifier.4.weight
Initializing omega values for layer classifier.4.bias
Epoch 1/100
--------------------
lr is 0.01
LR is set to 0.01
Loss: 0.0050 Acc: 0.4358

Epoch 2/100
--------------------
lr is 0.01
Loss: 0.0027 Acc: 0.7363

Epoch 3/100
--------------------
lr is 0.01
Loss: 0.0019 Acc: 0.8245

Epoch 4/100
--------------------
lr is 0.01
Loss: 0.0016 Acc: 0.8506

Epoch 5/100
--------------------
lr is 0.01
Loss: 0.0015 Acc: 0.8634

Epoch 6/100
--------------------
lr is 0.01
Loss: 0.0014 Acc: 0.8700

Epoch 7/100
--------------------
lr is 0.01
Loss: 0.0013 Acc: 0.8794

Epoch 8/100
--------------------
lr is 0.01
Loss: 0.0012 Acc: 0.8860

Epoch 9/100
--------------------
lr is 0.01
Loss: 0.0012 Acc: 0.8881

Epoch 10/100
--------------------
lr is 0.01
Loss: 0.0012 Acc: 0.8897

Epoch 11/100
--------------------
lr is 0.01
Loss: 0.0011 Acc: 0.8953

Epoch 12/100
--------------------
lr is 0.01
Loss: 0.0011 Acc: 0.8989

Epoch 13/100
--------------------
lr is 0.01
Loss: 0.0011 Acc: 0.9022

Epoch 14/100
--------------------
lr is 0.01
Loss: 0.0011 Acc: 0.9021

Epoch 15/100
--------------------
lr is 0.01
Loss: 0.0010 Acc: 0.9047

Epoch 16/100
--------------------
lr is 0.01
Loss: 0.0010 Acc: 0.9077

Epoch 17/100
--------------------
lr is 0.01
Loss: 0.0010 Acc: 0.9069

Epoch 18/100
--------------------
lr is 0.01
Loss: 0.0010 Acc: 0.9127

Epoch 19/100
--------------------
lr is 0.01
Loss: 0.0010 Acc: 0.9128

Epoch 20/100
--------------------
lr is 0.01
Loss: 0.0009 Acc: 0.9132

Epoch 21/100
--------------------
lr is 0.001
LR is set to 0.001
Loss: 0.0009 Acc: 0.9185

Epoch 22/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9182

Epoch 23/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9191

Epoch 24/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9190

Epoch 25/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9179

Epoch 26/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9202

Epoch 27/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9215

Epoch 28/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9204

Epoch 29/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9203

Epoch 30/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9203

Epoch 31/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9210

Epoch 32/100
--------------------
lr is 0.001
Loss: 0.0008 Acc: 0.9216

Epoch 33/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9210

Epoch 34/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9209

Epoch 35/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9200

Epoch 36/100
--------------------
lr is 0.001
Loss: 0.0008 Acc: 0.9226

Epoch 37/100
--------------------
lr is 0.001
Loss: 0.0008 Acc: 0.9221

Epoch 38/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9211

Epoch 39/100
--------------------
lr is 0.001
Loss: 0.0009 Acc: 0.9218

Epoch 40/100
--------------------
lr is 0.001
Loss: 0.0008 Acc: 0.9218

Epoch 41/100
--------------------
lr is 0.00010000000000000002
LR is set to 0.00010000000000000002
Loss: 0.0009 Acc: 0.9210

Epoch 42/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9226

Epoch 43/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9223

Epoch 44/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9223

Epoch 45/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9222

Epoch 46/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9217

Epoch 47/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9231

Epoch 48/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9220

Epoch 49/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9214

Epoch 50/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0009 Acc: 0.9218

Epoch 51/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9222

Epoch 52/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0009 Acc: 0.9215

Epoch 53/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9225

Epoch 54/100
--------------------
lr is 0.00010000000000000002
Loss: 0.0008 Acc: 0.9218

Epoch 55/100
--------------------
lr is 0.00010000000000000002
Matplotlib created a temporary config/cache directory at /tmp/matplotlib-92qkwbt5 because the default path (/home/20/xuanming/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Training the model on task 1
Initializing omega values for layer features.8.weight
Initializing omega values for layer features.8.bias
Initializing omega values for layer features.10.weight
Initializing omega values for layer features.10.bias
Initializing omega values for layer classifier.1.weight
Initializing omega values for layer classifier.1.bias
Initializing omega values for layer classifier.4.weight
Initializing omega values for layer classifier.4.bias
Epoch 1/10
--------------------
lr is 0.1
LR is set to 0.1
lr is 0.1
LR is set to 0.1
Loss: 0.0039 Acc: 0.5841

Epoch 2/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0021 Acc: 0.7997

Epoch 3/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0016 Acc: 0.8495

Epoch 4/10
--------------------
lr is 0.1
lr is 0.1
Matplotlib created a temporary config/cache directory at /tmp/matplotlib-h9sd9uf8 because the default path (/home/20/xuanming/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Training the model on task 1
Initializing omega values for layer features.8.weight
Initializing omega values for layer features.8.bias
Initializing omega values for layer features.10.weight
Initializing omega values for layer features.10.bias
Initializing omega values for layer classifier.1.weight
Initializing omega values for layer classifier.1.bias
Initializing omega values for layer classifier.4.weight
Initializing omega values for layer classifier.4.bias
Epoch 1/10
--------------------
lr is 0.1
LR is set to 0.1
lr is 0.1
LR is set to 0.1
Loss: 0.0042 Acc: 0.5454

Epoch 2/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0023 Acc: 0.7838

Epoch 3/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0018 Acc: 0.8312

Epoch 4/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0016 Acc: 0.8518

Epoch 5/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0015 Acc: 0.8631

Epoch 6/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0013 Acc: 0.8724

Epoch 7/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0013 Acc: 0.8793

Epoch 8/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0012 Acc: 0.8876

Epoch 9/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0011 Acc: 0.8944

Epoch 10/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0011 Acc: 0.8959

Updating the omega values for this task
Training the model on task 2
Epoch 1/10
--------------------
lr is 0.1
LR is set to 0.1
lr is 0.1
LR is set to 0.1
Loss: 0.0044 Acc: 0.5572

Epoch 2/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0032 Acc: 0.6906

Epoch 3/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0027 Acc: 0.7424

Epoch 4/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0024 Acc: 0.7702

Epoch 5/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0023 Acc: 0.7834

Epoch 6/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0021 Acc: 0.8012

Epoch 7/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0020 Acc: 0.8117

Epoch 8/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0019 Acc: 0.8194

Epoch 9/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0018 Acc: 0.8277

Epoch 10/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0018 Acc: 0.8348

Updating the omega values for this task
Training the model on task 3
Epoch 1/10
--------------------
lr is 0.1
LR is set to 0.1
lr is 0.1
LR is set to 0.1
Loss: 0.0031 Acc: 0.7703

Epoch 2/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0018 Acc: 0.8512

Epoch 3/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0016 Acc: 0.8679

Epoch 4/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0014 Acc: 0.8809

Epoch 5/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0013 Acc: 0.8935

Epoch 6/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0012 Acc: 0.9023

Epoch 7/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0011 Acc: 0.9079

Epoch 8/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0010 Acc: 0.9153

Epoch 9/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0010 Acc: 0.9190

Epoch 10/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0009 Acc: 0.9225

Updating the omega values for this task
Training the model on task 4
Epoch 1/10
--------------------
lr is 0.1
LR is set to 0.1
lr is 0.1
LR is set to 0.1
Loss: 0.0042 Acc: 0.6909

Epoch 2/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0026 Acc: 0.7921

Epoch 3/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0023 Acc: 0.8119

Epoch 4/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0021 Acc: 0.8281

Epoch 5/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0019 Acc: 0.8413

Epoch 6/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0018 Acc: 0.8538

Epoch 7/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0017 Acc: 0.8601

Epoch 8/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0016 Acc: 0.8665

Epoch 9/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0016 Acc: 0.8701

Epoch 10/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0015 Acc: 0.8763

Updating the omega values for this task
Training the model on task 5
Epoch 1/10
--------------------
lr is 0.1
LR is set to 0.1
lr is 0.1
LR is set to 0.1
Loss: 0.0181 Acc: 0.0965

Epoch 2/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0097 Acc: 0.4141

Epoch 3/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0055 Acc: 0.6343

Epoch 4/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0036 Acc: 0.7503

Epoch 5/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0027 Acc: 0.8055

Epoch 6/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0021 Acc: 0.8472

Epoch 7/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0016 Acc: 0.8754

Epoch 8/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0013 Acc: 0.8974

Epoch 9/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0011 Acc: 0.9130

Epoch 10/10
--------------------
lr is 0.1
lr is 0.1
Loss: 0.0009 Acc: 0.9316

Updating the omega values for this task
The training process on the 5 tasks is completed
Testing the model now
Testing the model on task 1
The forgetting undergone on task 1 is -42.5660%
Testing the model on task 2
The forgetting undergone on task 2 is -42.4151%
Testing the model on task 3
The forgetting undergone on task 3 is -46.7925%
Testing the model on task 4
The forgetting undergone on task 4 is -57.5094%
Testing the model on task 5
The forgetting undergone on task 5 is 0.0000%
