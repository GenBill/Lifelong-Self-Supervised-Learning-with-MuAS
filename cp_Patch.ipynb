{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, datasets\n",
    " \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    " \n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    " \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# Rotation\n",
    "# Saliency Check\n",
    "# With color shift or grayscale conversion\n",
    "\n",
    "#########################################\n",
    "# Parameters \n",
    "#########################################\n",
    "\n",
    "# training_image_paths = glob('/data/mini-open-images-dataset/train/*.jpg')\n",
    "# validation_image_paths = glob('/data/mini-open-images-dataset/validation/*.jpg')\n",
    "training_image_paths = glob('./cifar_10/train/*.jpg')       # glob('./Kaggle256/train/*.jpg')\n",
    "validation_image_paths = glob('./cifar_10/train/*.jpg')     # glob('./Kaggle256/valid/*.jpg')\n",
    "\n",
    "train_dataset_length = 52000        # 40960\n",
    "validation_dataset_length = 52000   # 40960\n",
    "train_batch_size = 128\n",
    "validation_batch_size = 128\n",
    "num_epochs = 10\n",
    "backup_after_epochs = 10 \n",
    "model_save_prefix = \"rotation_jigsaw\"\n",
    "color_shift = 1\n",
    "\n",
    "patch_dim = 14\n",
    "jitter = 0\n",
    "gray_portion = .30\n",
    "reuse_image_count = 4\n",
    "\n",
    "learn_rate = 0.001\n",
    "momentum = 0.9  # 74\n",
    "weight_decay = 0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# This class generates patches for training\n",
    "#########################################\n",
    "\n",
    "patch_order_arr = [\n",
    "  (0, 1, 2, 3),\n",
    "  (0, 1, 3, 2),\n",
    "  (0, 2, 1, 3),\n",
    "  (0, 2, 3, 1),\n",
    "  (0, 3, 1, 2),\n",
    "  (0, 3, 2, 1),\n",
    "  (1, 0, 2, 3),\n",
    "  (1, 0, 3, 2),\n",
    "  (1, 2, 0, 3),\n",
    "  (1, 2, 3, 0),\n",
    "  (1, 3, 0, 2),\n",
    "  (1, 3, 2, 0),\n",
    "  (2, 0, 1, 3),\n",
    "  (2, 0, 3, 1),\n",
    "  (2, 1, 0, 3),\n",
    "  (2, 1, 3, 0),\n",
    "  (2, 3, 0, 1),\n",
    "  (2, 3, 1, 0),\n",
    "  (3, 0, 1, 2),\n",
    "  (3, 0, 2, 1),\n",
    "  (3, 1, 0, 2),\n",
    "  (3, 1, 2, 0),\n",
    "  (3, 2, 0, 1),\n",
    "  (3, 2, 1, 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShufflePatchDataset(Dataset):\n",
    "\n",
    "  def __init__(self, image_paths, patch_dim, length, jitter, color_shift, transform=None):\n",
    "    self.image_paths = image_paths\n",
    "    self.patch_dim = patch_dim\n",
    "    self.length = length\n",
    "    self.jitter = jitter\n",
    "    self.color_shift = color_shift\n",
    "    self.transform = transform\n",
    "    self.image_reused = 0\n",
    "\n",
    "    self.sub_window_width = self.patch_dim + 2*self.jitter + 2*self.color_shift\n",
    "    self.window_width = 2*self.sub_window_width\n",
    "    \n",
    "    self.min_image_width = self.window_width + 1\n",
    "\n",
    "    self.saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "  def random_jitter(self):\n",
    "    return int(math.floor((self.jitter * 2 * random.random())))\n",
    "\n",
    "  def random_shift(self):\n",
    "    return random.randrange(self.color_shift * 2 + 1)\n",
    "\n",
    "  def prep_patch(self, image):\n",
    " \n",
    "    cropped = np.empty((self.patch_dim, self.patch_dim, 3), dtype=np.uint8)\n",
    "\n",
    "    if(random.random() < gray_portion):\n",
    "\n",
    "      pil_patch = Image.fromarray(image)\n",
    "      pil_patch = pil_patch.convert('L')\n",
    "      pil_patch = pil_patch.convert('RGB')\n",
    "      np.copyto(cropped, np.array(pil_patch)[self.color_shift:self.color_shift+self.patch_dim, self.color_shift:self.color_shift+self.patch_dim, :])\n",
    "      \n",
    "    else:\n",
    "\n",
    "      shift = [self.random_shift() for _ in range(6)]\n",
    "      cropped[:,:,0] = image[shift[0]:shift[0]+self.patch_dim, shift[1]:shift[1]+self.patch_dim, 0]\n",
    "      cropped[:,:,1] = image[shift[2]:shift[2]+self.patch_dim, shift[3]:shift[3]+self.patch_dim, 1]\n",
    "      cropped[:,:,2] = image[shift[4]:shift[4]+self.patch_dim, shift[5]:shift[5]+self.patch_dim, 2]\n",
    "\n",
    "    return cropped\n",
    "\n",
    "  def saliency_check(self, window, patch_coords):\n",
    "    (success, saliency_map) = self.saliency.computeSaliency(cv2.cvtColor(window, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    high_saliency_patches = 0\n",
    "    med_saliency_patches = 0\n",
    "    \n",
    "    for p in patch_coords:\n",
    "        patch_saliency_map = saliency_map[p[0]:p[0]+self.patch_dim, p[1]:p[1]+self.patch_dim]\n",
    "        patch_saliency = np.sum(patch_saliency_map > .5)\n",
    "        if patch_saliency >= 400:\n",
    "          high_saliency_patches += 1\n",
    "        elif patch_saliency >= 150:\n",
    "          med_saliency_patches += 1\n",
    "\n",
    "    return high_saliency_patches > 0 and (high_saliency_patches + med_saliency_patches) > 2\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # [y, x, chan], dtype=uint8, top_left is (0,0)\n",
    "    \n",
    "    image_index = int(math.floor((len(self.image_paths) * random.random())))\n",
    "    \n",
    "    if self.image_reused == 0:\n",
    "      self.pil_image = Image.open(self.image_paths[image_index]).convert('RGB')\n",
    "      self.image_reused = reuse_image_count - 1\n",
    "    else:\n",
    "      self.image_reused -= 1\n",
    "\n",
    "    image = np.array(self.pil_image)\n",
    "\n",
    "    # If image is too small, try another image\n",
    "    '''\n",
    "    if (image.shape[0] - self.min_image_width) <= 0 or (image.shape[1] - self.min_image_width) <= 0:\n",
    "        return self.__getitem__(index)\n",
    "    '''\n",
    "    window_y_coord = int(math.floor((image.shape[0] - self.window_width) * random.random()))\n",
    "    window_x_coord = int(math.floor((image.shape[1] - self.window_width) * random.random()))\n",
    "\n",
    "    window = image[window_y_coord:window_y_coord+self.window_width, window_x_coord:window_x_coord+self.window_width]\n",
    "    \n",
    "    rotation_label = int(math.floor((4 * random.random())))\n",
    "    order_label = int(math.floor((24 * random.random()))) \n",
    "    \n",
    "    if rotation_label>0:\n",
    "      window = np.rot90(window, rotation_label).copy()\n",
    "\n",
    "    patch_coords = [\n",
    "      (self.random_jitter(), self.random_jitter()),\n",
    "      (self.random_jitter(), self.sub_window_width + self.random_jitter()),\n",
    "      (self.sub_window_width + self.random_jitter(), self.random_jitter()),\n",
    "      (self.sub_window_width + self.random_jitter(), self.sub_window_width + self.random_jitter()),\n",
    "    ]\n",
    "\n",
    "    patch_coords = [pc for _,pc in sorted(zip(patch_order_arr[order_label],patch_coords))]\n",
    "\n",
    "    if not self.saliency_check(window, patch_coords):\n",
    "      return self.__getitem__(index)\n",
    "\n",
    "    patch_a = window[patch_coords[0][0]:patch_coords[0][0]+self.patch_dim+2*self.color_shift, patch_coords[0][1]:patch_coords[0][1]+self.patch_dim+2*self.color_shift]\n",
    "    patch_b = window[patch_coords[1][0]:patch_coords[1][0]+self.patch_dim+2*self.color_shift, patch_coords[1][1]:patch_coords[1][1]+self.patch_dim+2*self.color_shift]\n",
    "    patch_c = window[patch_coords[2][0]:patch_coords[2][0]+self.patch_dim+2*self.color_shift, patch_coords[2][1]:patch_coords[2][1]+self.patch_dim+2*self.color_shift]\n",
    "    patch_d = window[patch_coords[3][0]:patch_coords[3][0]+self.patch_dim+2*self.color_shift, patch_coords[3][1]:patch_coords[3][1]+self.patch_dim+2*self.color_shift]\n",
    "\n",
    "    # gray = random.random() < gray_portion\n",
    "\n",
    "    patch_a = self.prep_patch(patch_a)\n",
    "    patch_b = self.prep_patch(patch_b)\n",
    "    patch_c = self.prep_patch(patch_c)\n",
    "    patch_d = self.prep_patch(patch_d)\n",
    "\n",
    "    combined_label = np.array(rotation_label * 24 + order_label).astype(np.int64)\n",
    "        \n",
    "    if self.transform:\n",
    "      patch_a = self.transform(patch_a)\n",
    "      patch_b = self.transform(patch_b)\n",
    "      patch_c = self.transform(patch_c)\n",
    "      patch_d = self.transform(patch_d)\n",
    "\n",
    "    return patch_a, patch_b, patch_c, patch_d, combined_label\n",
    "    \n",
    "\n",
    "##################################################\n",
    "# Creating Train/Validation dataset and dataloader\n",
    "##################################################\n",
    "\n",
    "traindataset = ShufflePatchDataset(training_image_paths, patch_dim, train_dataset_length, jitter, color_shift,\n",
    "                         transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindataset, \n",
    "                                          batch_size=train_batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "valdataset = ShufflePatchDataset(validation_image_paths, patch_dim, validation_dataset_length, jitter, color_shift,\n",
    "                         transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valdataset,\n",
    "                                        batch_size=validation_batch_size,\n",
    "                                        num_workers=4,\n",
    "                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 12, 12]           1,792\n       BatchNorm2d-2           [-1, 64, 12, 12]             128\n              ReLU-3           [-1, 64, 12, 12]               0\n            Conv2d-4           [-1, 64, 10, 10]          36,928\n       BatchNorm2d-5           [-1, 64, 10, 10]             128\n              ReLU-6           [-1, 64, 10, 10]               0\n            Conv2d-7            [-1, 128, 8, 8]          73,856\n       BatchNorm2d-8            [-1, 128, 8, 8]             256\n              ReLU-9            [-1, 128, 8, 8]               0\n           Conv2d-10            [-1, 512, 6, 6]         590,336\n      BatchNorm2d-11            [-1, 512, 6, 6]           1,024\n             ReLU-12            [-1, 512, 6, 6]               0\n        MaxPool2d-13            [-1, 512, 3, 3]               0\n           Linear-14                 [-1, 4096]      18,878,464\n             ReLU-15                 [-1, 4096]               0\n          Dropout-16                 [-1, 4096]               0\n           Conv2d-17           [-1, 64, 12, 12]           1,792\n      BatchNorm2d-18           [-1, 64, 12, 12]             128\n             ReLU-19           [-1, 64, 12, 12]               0\n           Conv2d-20           [-1, 64, 10, 10]          36,928\n      BatchNorm2d-21           [-1, 64, 10, 10]             128\n             ReLU-22           [-1, 64, 10, 10]               0\n           Conv2d-23            [-1, 128, 8, 8]          73,856\n      BatchNorm2d-24            [-1, 128, 8, 8]             256\n             ReLU-25            [-1, 128, 8, 8]               0\n           Conv2d-26            [-1, 512, 6, 6]         590,336\n      BatchNorm2d-27            [-1, 512, 6, 6]           1,024\n             ReLU-28            [-1, 512, 6, 6]               0\n        MaxPool2d-29            [-1, 512, 3, 3]               0\n           Linear-30                 [-1, 4096]      18,878,464\n             ReLU-31                 [-1, 4096]               0\n          Dropout-32                 [-1, 4096]               0\n           Conv2d-33           [-1, 64, 12, 12]           1,792\n      BatchNorm2d-34           [-1, 64, 12, 12]             128\n             ReLU-35           [-1, 64, 12, 12]               0\n           Conv2d-36           [-1, 64, 10, 10]          36,928\n      BatchNorm2d-37           [-1, 64, 10, 10]             128\n             ReLU-38           [-1, 64, 10, 10]               0\n           Conv2d-39            [-1, 128, 8, 8]          73,856\n      BatchNorm2d-40            [-1, 128, 8, 8]             256\n             ReLU-41            [-1, 128, 8, 8]               0\n           Conv2d-42            [-1, 512, 6, 6]         590,336\n      BatchNorm2d-43            [-1, 512, 6, 6]           1,024\n             ReLU-44            [-1, 512, 6, 6]               0\n        MaxPool2d-45            [-1, 512, 3, 3]               0\n           Linear-46                 [-1, 4096]      18,878,464\n             ReLU-47                 [-1, 4096]               0\n          Dropout-48                 [-1, 4096]               0\n           Conv2d-49           [-1, 64, 12, 12]           1,792\n      BatchNorm2d-50           [-1, 64, 12, 12]             128\n             ReLU-51           [-1, 64, 12, 12]               0\n           Conv2d-52           [-1, 64, 10, 10]          36,928\n      BatchNorm2d-53           [-1, 64, 10, 10]             128\n             ReLU-54           [-1, 64, 10, 10]               0\n           Conv2d-55            [-1, 128, 8, 8]          73,856\n      BatchNorm2d-56            [-1, 128, 8, 8]             256\n             ReLU-57            [-1, 128, 8, 8]               0\n           Conv2d-58            [-1, 512, 6, 6]         590,336\n      BatchNorm2d-59            [-1, 512, 6, 6]           1,024\n             ReLU-60            [-1, 512, 6, 6]               0\n        MaxPool2d-61            [-1, 512, 3, 3]               0\n           Linear-62                 [-1, 4096]      18,878,464\n             ReLU-63                 [-1, 4096]               0\n          Dropout-64                 [-1, 4096]               0\n           Linear-65                 [-1, 4096]      67,112,960\n             ReLU-66                 [-1, 4096]               0\n          Dropout-67                 [-1, 4096]               0\n           Linear-68                   [-1, 96]         393,312\n================================================================\nTotal params: 145,837,920\nTrainable params: 145,837,920\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 456004.77\nForward/backward pass size (MB): 4.48\nParams size (MB): 556.33\nEstimated Total Size (MB): 456565.57\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##################################################\n",
    "# Model for learning patch position\n",
    "##################################################\n",
    "\n",
    "class VggNetwork(nn.Module):\n",
    "  def __init__(self,aux_logits = False):\n",
    "\n",
    "      super(VggNetwork, self).__init__()\n",
    "\n",
    "      self.cnn = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "      )\n",
    "    \n",
    "      self.fc6 = nn.Sequential(\n",
    "        nn.Linear(512 * 3 * 3, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "      )\n",
    "\n",
    "      self.fc = nn.Sequential(\n",
    "        nn.Linear(4*4096, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096, 96),\n",
    "      )\n",
    "\n",
    "      self.cnn6 = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(128, 512, kernel_size=3, padding=0),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      )\n",
    "\n",
    "  def forward_once(self, x):\n",
    "    output= self.cnn6(x)\n",
    "    output = output.view(output.size()[0], -1)\n",
    "    output = self.fc6(output)\n",
    "    return output\n",
    "\n",
    "  def forward(self, patch_a, patch_b, patch_c, patch_d):\n",
    "    output_fc6_patch_a = self.forward_once(patch_a)\n",
    "    output_fc6_patch_b = self.forward_once(patch_b)\n",
    "    output_fc6_patch_c = self.forward_once(patch_c)\n",
    "    output_fc6_patch_d = self.forward_once(patch_d)\n",
    "\n",
    "    output = torch.cat((output_fc6_patch_a, output_fc6_patch_b, output_fc6_patch_c, output_fc6_patch_d), 1)\n",
    "    output = self.fc(output)\n",
    "\n",
    "    return output, output_fc6_patch_a, output_fc6_patch_b, output_fc6_patch_c, output_fc6_patch_d\n",
    "\n",
    "model = VggNetwork().to(device)\n",
    "summary(model, [(3, patch_dim, patch_dim), (3, patch_dim, patch_dim), (3, patch_dim, patch_dim), (3, patch_dim, patch_dim)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Initialized Optimizer, criterion, scheduler\n",
    "#############################################\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "  model.parameters(), \n",
    "  lr=learn_rate,\n",
    "  momentum=momentum,\n",
    "  weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Load Checkpoint\n",
    "#############################################\n",
    "\n",
    "global_trn_loss = []\n",
    "global_val_loss = []\n",
    "\n",
    "last_epoch = -1\n",
    "\n",
    "training_image_paths = glob(f'{model_save_prefix}_*.pt')\n",
    "\n",
    "if len(training_image_paths) > 0:\n",
    "  training_image_paths.sort()  \n",
    "  model_save_path = training_image_paths[-1]\n",
    "  try:\n",
    "    print('Loading Checkpoint...', model_save_path)\n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    last_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    global_trn_loss = checkpoint['global_trnloss']\n",
    "    global_val_loss = checkpoint['global_valloss']\n",
    "  except:\n",
    "    print(\"Loading Checkpoint Failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "starting train loop (b.0)\n",
      "epoch 0\n",
      "  0%|          | 0/406 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RecursionError",
     "evalue": "Caught RecursionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-15-2cc4524c429f>\", line 104, in __getitem__\n    return self.__getitem__(index)\n  File \"<ipython-input-15-2cc4524c429f>\", line 104, in __getitem__\n    return self.__getitem__(index)\n  File \"<ipython-input-15-2cc4524c429f>\", line 104, in __getitem__\n    return self.__getitem__(index)\n  [Previous line repeated 2932 more times]\n  File \"<ipython-input-15-2cc4524c429f>\", line 92, in __getitem__\n    window = np.rot90(window, rotation_label).copy()\n  File \"<__array_function__ internals>\", line 5, in rot90\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\", line 140, in rot90\n    return transpose(flip(m, axes[1]), axes_list)\n  File \"<__array_function__ internals>\", line 5, in flip\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\", line 236, in flip\n    axis = _nx.normalize_axis_tuple(axis, m.ndim)\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/core/numeric.py\", line 1358, in normalize_axis_tuple\n    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/core/numeric.py\", line 1358, in <listcomp>\n    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\nRecursionError: maximum recursion depth exceeded while calling a Python object\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-245e5decfcc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m## Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# for idx, data in enumerate(trainloader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpatch_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_shuffle_order_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: Caught RecursionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-15-2cc4524c429f>\", line 104, in __getitem__\n    return self.__getitem__(index)\n  File \"<ipython-input-15-2cc4524c429f>\", line 104, in __getitem__\n    return self.__getitem__(index)\n  File \"<ipython-input-15-2cc4524c429f>\", line 104, in __getitem__\n    return self.__getitem__(index)\n  [Previous line repeated 2932 more times]\n  File \"<ipython-input-15-2cc4524c429f>\", line 92, in __getitem__\n    window = np.rot90(window, rotation_label).copy()\n  File \"<__array_function__ internals>\", line 5, in rot90\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\", line 140, in rot90\n    return transpose(flip(m, axes[1]), axes_list)\n  File \"<__array_function__ internals>\", line 5, in flip\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\", line 236, in flip\n    axis = _nx.normalize_axis_tuple(axis, m.ndim)\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/core/numeric.py\", line 1358, in normalize_axis_tuple\n    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\n  File \"/home/zhangxuanming/anaconda3/lib/python3.8/site-packages/numpy/core/numeric.py\", line 1358, in <listcomp>\n    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\nRecursionError: maximum recursion depth exceeded while calling a Python object\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Training/Validation Engine\n",
    "############################\n",
    "\n",
    "print(\"starting train loop (b.0)\")\n",
    "\n",
    "for epoch in range(last_epoch+1, num_epochs):\n",
    "    print(\"epoch\", epoch)\n",
    "\n",
    "    train_running_loss = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    ## Train \n",
    "    for idx, data in tqdm(enumerate(trainloader), total=int(len(traindataset)/train_batch_size)):\n",
    "    # for idx, data in enumerate(trainloader):\n",
    "        patch_a, patch_b, patch_c, patch_d, patch_shuffle_order_label = data[0].to(device), data[1].to(device), data[2].to(device), data[3].to(device), data[4].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, output_fc6_patch_a, output_fc6_patch_b, output_fc6_patch_c, output_fc6_patch_d = model(patch_a, patch_b, patch_c, patch_d)\n",
    "        loss = criterion(output, patch_shuffle_order_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss.append(loss.item())\n",
    "  \n",
    "    global_trn_loss.append(sum(train_running_loss) / len(train_running_loss))\n",
    "\n",
    "\n",
    "    ## Validation\n",
    "\n",
    "    if epoch % backup_after_epochs == 0:\n",
    "      val_running_loss = []\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      model.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for idx, data in tqdm(enumerate(valloader), total=int(len(valdataset)/validation_batch_size)):\n",
    "          patch_a, patch_b, patch_c, patch_d, patch_shuffle_order_label = data[0].to(device), data[1].to(device), data[2].to(device), data[3].to(device), data[4].to(device)\n",
    "          output, output_fc6_patch_a, output_fc6_patch_b, output_fc6_patch_c, output_fc6_patch_d = model(patch_a, patch_b, patch_c, patch_d)\n",
    "          loss = criterion(output, patch_shuffle_order_label)\n",
    "          val_running_loss.append(loss.item())\n",
    "        \n",
    "          _, predicted = torch.max(output.data, 1)\n",
    "          total += patch_shuffle_order_label.size(0)\n",
    "          correct += (predicted == patch_shuffle_order_label).sum()\n",
    "        print('Val Progress --- total:{}, correct:{}'.format(total, correct.item()))\n",
    "        print('Val Accuracy of the network on the test images: {}%'.format(100 * correct.item() / total))\n",
    "\n",
    "      global_val_loss.append(sum(val_running_loss) / len(val_running_loss))\n",
    "\n",
    "    else:\n",
    "      if len(global_val_loss) > 0:\n",
    "        global_val_loss.append(global_val_loss[-1])\n",
    "      else:\n",
    "        global_val_loss.append(0)\n",
    "    \n",
    "    \n",
    "    print('Epoch [{}/{}], TRNLoss:{:.4f}, VALLoss:{:.4f}, Time:{:.2f}'.format(\n",
    "        epoch + 1, num_epochs, global_trn_loss[-1], global_val_loss[-1],\n",
    "        (time.time() - start_time) / 60))\n",
    "    \n",
    "    # delete old images\n",
    "    training_image_paths = glob(f'{model_save_prefix}_*.pt')\n",
    "    if len(training_image_paths) > 2:\n",
    "      training_image_paths.sort()\n",
    "      for i in range(len(training_image_paths)-2):\n",
    "        training_image_path = training_image_paths[i]\n",
    "        os.remove(training_image_path)\n",
    "\n",
    "    # save new image\n",
    "    model_save_path = f'{model_save_prefix}_{epoch:04d}.pt'\n",
    "    print('saving checkpoint', model_save_path)\n",
    "    torch.save(\n",
    "      {\n",
    "          'epoch': epoch,\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "          'loss': loss,\n",
    "          'global_trnloss': global_trn_loss,\n",
    "          'global_valloss': global_val_loss\n",
    "      }, model_save_path\n",
    "    )\n",
    "  \n",
    "    if epoch % backup_after_epochs == 0:\n",
    "      print('backing up checkpoint', model_save_path)\n",
    "      os.system(f'aws s3 cp /data/{model_save_path} s3://guiuan/{model_save_prefix}_{epoch:04d}_{learn_rate}_{global_trn_loss[-1]:.4f}_{(100 * correct.item()/total):.2f}.pt')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}